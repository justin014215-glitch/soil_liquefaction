import os
import tempfile
import zipfile
import shutil
from datetime import datetime
from typing import Optional, Dict, Any, Tuple
import pandas as pd
import geopandas as gpd
from pathlib import Path

from django.conf import settings
from django.core.files.storage import default_storage
from django.core.files.base import ContentFile

from .HBF import HBF
from .NCEER import NCEER

class LiquefactionAnalyzer:
    """æ¶²åŒ–åˆ†æå¼•æ“ - æ•´åˆ HBF å’Œ NCEER æ–¹æ³•"""
    
    def __init__(self):
        self.supported_methods = {
            'HBF': 'HBF (2012) æ–¹æ³•',
            'NCEER': 'NCEER (2001) æ–¹æ³•'
        }
        self.results_base_dir = getattr(settings, 'LIQUEFACTION_RESULTS_DIR', 
                                       os.path.join(settings.MEDIA_ROOT, 'liquefaction_results'))
        
    def create_analysis_directory(self, method: str, timestamp: str = None) -> str:
        """å‰µå»ºåˆ†æçµæœç›®éŒ„"""
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å‰µå»ºä¸»ç›®éŒ„çµæ§‹ï¼šliquefaction_results/method_timestamp/
        analysis_dir = os.path.join(
            self.results_base_dir,
            f"{method}_{timestamp}"
        )
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(analysis_dir, exist_ok=True)
        
        # å‰µå»ºå­ç›®éŒ„
        subdirs = [
            'raw_results',      # åŸå§‹åˆ†æçµæœ
            'simplified_reports', # ç°¡åŒ–å ±è¡¨
            'individual_wells',  # å€‹åˆ¥é‘½å­”è³‡æ–™å¤¾
            'charts',           # åœ–è¡¨
            'summary'           # æ‘˜è¦å ±è¡¨
        ]
        
        for subdir in subdirs:
            os.makedirs(os.path.join(analysis_dir, subdir), exist_ok=True)
            
        return analysis_dir

    def prepare_input_data(self, csv_content: bytes, method: str) -> Tuple[str, Optional[str]]:
        """æº–å‚™è¼¸å…¥è³‡æ–™ä¸¦é€²è¡Œåˆæ­¥é©—è­‰"""
        try:
            # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
            with tempfile.NamedTemporaryFile(mode='wb', suffix='.csv', delete=False) as temp_file:
                temp_file.write(csv_content)
                temp_csv_path = temp_file.name
            
            # é©—è­‰CSVæ ¼å¼
            try:
                df = pd.read_csv(temp_csv_path)
                print(f"æˆåŠŸè®€å–CSVæª”æ¡ˆï¼Œå…± {len(df)} ç­†è³‡æ–™")
                
                # åŸºæœ¬æ¬„ä½æª¢æŸ¥
                required_columns = ['é‘½å­”ç·¨è™Ÿ', 'TWD97_X', 'TWD97_Y']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    error_msg = f"CSVæª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}"
                    print(f"âŒ {error_msg}")
                    os.unlink(temp_csv_path)
                    return None, error_msg
                
                print(f"âœ… CSVæª”æ¡ˆæ ¼å¼é©—è­‰é€šé")
                return temp_csv_path, None
                
            except Exception as e:
                error_msg = f"CSVæª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼š{str(e)}"
                print(f"âŒ {error_msg}")
                os.unlink(temp_csv_path)
                return None, error_msg
                
        except Exception as e:
            error_msg = f"è™•ç†è¼¸å…¥æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            print(f"âŒ {error_msg}")
            return None, error_msg

    def prepare_fault_data(self, shapefile_content: bytes = None) -> Optional[gpd.GeoDataFrame]:
        """æº–å‚™æ–·å±¤è³‡æ–™"""
        if shapefile_content is None:
            print("æœªæä¾›æ–·å±¤è³‡æ–™ï¼Œè·³éæ–·å±¤åˆ†æ")
            return None
            
        try:
            # å‰µå»ºè‡¨æ™‚ç›®éŒ„ä¾†è§£å£“ç¸®shapefile
            with tempfile.TemporaryDirectory() as temp_dir:
                # å‡è¨­å‚³å…¥çš„æ˜¯zipæª”æ¡ˆåŒ…å«å®Œæ•´çš„shapefile
                zip_path = os.path.join(temp_dir, 'fault_data.zip')
                with open(zip_path, 'wb') as f:
                    f.write(shapefile_content)
                
                # è§£å£“ç¸®
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                
                # å°‹æ‰¾.shpæª”æ¡ˆ
                shp_files = list(Path(temp_dir).glob('**/*.shp'))
                if not shp_files:
                    print("âŒ åœ¨ä¸Šå‚³çš„æª”æ¡ˆä¸­æ‰¾ä¸åˆ°.shpæª”æ¡ˆ")
                    return None
                
                shp_path = str(shp_files[0])
                fault_gdf = gpd.read_file(shp_path)
                print(f"âœ… æˆåŠŸè¼‰å…¥æ–·å±¤è³‡æ–™ï¼š{len(fault_gdf)} å€‹è¨˜éŒ„")
                return fault_gdf
                
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ–·å±¤è³‡æ–™å¤±æ•—ï¼š{e}")
            return None

    def analyze_hbf(self, 
                   csv_path: str, 
                   output_dir: str,
                   fault_gdf: Optional[gpd.GeoDataFrame] = None,
                   em_value: float = 72,
                   unit_weight_unit: str = "t/m3") -> Tuple[Optional[pd.DataFrame], Optional[Dict], Optional[str]]:
        """åŸ·è¡Œ HBF åˆ†æ"""
        print("\n" + "="*60)
        print("é–‹å§‹ HBF (2012) æ¶²åŒ–åˆ†æ")
        print("="*60)
        
        try:
            # åˆå§‹åŒ–HBFåˆ†æå™¨
            if unit_weight_unit.lower() == "kn/m3":
                unit_weight_conversion_factor = 1.0/9.81
            else:
                unit_weight_conversion_factor = 1.0
                
            hbf_analyzer = HBF(
                default_em=em_value,
                unit_weight_conversion_factor=unit_weight_conversion_factor
            )
            
            # åŸ·è¡Œåˆ†æ
            final_df, lpi_summary, _ = hbf_analyzer.HBF_main(
                show_gui=False,
                input_file_path=csv_path,
                output_file_path=os.path.join(output_dir, 'raw_results', 'HBF_complete_results.csv'),
                use_fault_data=(fault_gdf is not None),
                fault_shapefile_path=None,
                custom_em=em_value,
                unit_weight_unit=unit_weight_unit
            )
            
            if final_df is not None:
                print("âœ… HBF åˆ†æå®Œæˆ")
                return final_df, lpi_summary, None
            else:
                error_msg = "HBF åˆ†æå¤±æ•—ï¼šæœªç”¢ç”Ÿåˆ†æçµæœ"
                print(f"âŒ {error_msg}")
                return None, None, error_msg
                
        except Exception as e:
            error_msg = f"HBF åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤ï¼š{traceback.format_exc()}")
            return None, None, error_msg

    def analyze_nceer(self, 
                     csv_path: str, 
                     output_dir: str,
                     fault_gdf: Optional[gpd.GeoDataFrame] = None,
                     em_value: float = 72) -> Tuple[Optional[pd.DataFrame], Optional[Dict], Optional[str]]:
        """åŸ·è¡Œ NCEER åˆ†æ"""
        print("\n" + "="*60)
        print("é–‹å§‹ NCEER (2001) æ¶²åŒ–åˆ†æ")
        print("="*60)
        
        try:
            # åˆå§‹åŒ–NCEERåˆ†æå™¨
            nceer_analyzer = NCEER(default_em=em_value)
            
            # åŸ·è¡Œåˆ†æ
            final_df, lpi_summary, _ = nceer_analyzer.NCEER_main(
                show_gui=False,
                input_file_path=csv_path,
                output_file_path=os.path.join(output_dir, 'raw_results', 'NCEER_complete_results.csv'),
                use_fault_data=(fault_gdf is not None),
                fault_shapefile_path=None,
                custom_em=em_value
            )
            
            if final_df is not None:
                print("âœ… NCEER åˆ†æå®Œæˆ")
                return final_df, lpi_summary, None
            else:
                error_msg = "NCEER åˆ†æå¤±æ•—ï¼šæœªç”¢ç”Ÿåˆ†æçµæœ"
                print(f"âŒ {error_msg}")
                return None, None, error_msg
                
        except Exception as e:
            error_msg = f"NCEER åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤ï¼š{traceback.format_exc()}")
            return None, None, error_msg

    def generate_simplified_reports(self, 
                                  analyzer, 
                                  final_df: pd.DataFrame, 
                                  output_dir: str) -> Dict[str, str]:
        """ç”Ÿæˆç°¡åŒ–å ±è¡¨"""
        print("\n=== ç”Ÿæˆç°¡åŒ–å ±è¡¨ ===")
        
        simplified_reports = {}
        scenarios = ["Design", "MidEq", "MaxEq"]
        
        simplified_dir = os.path.join(output_dir, 'simplified_reports')
        
        for scenario in scenarios:
            try:
                report_file = analyzer.generate_simplified_report(
                    final_df, 
                    output_dir=simplified_dir, 
                    scenario=scenario
                )
                if report_file:
                    simplified_reports[scenario] = report_file
                    print(f"âœ… {scenario} æƒ…å¢ƒç°¡åŒ–å ±è¡¨ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆ {scenario} æƒ…å¢ƒç°¡åŒ–å ±è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        return simplified_reports

    def generate_individual_well_reports(self, 
                                       final_df: pd.DataFrame, 
                                       output_dir: str) -> Dict[str, str]:
        """ç‚ºæ¯å€‹é‘½å­”ç”Ÿæˆç¨ç«‹å ±è¡¨å’Œåœ–è¡¨"""
        print("\n=== ç”Ÿæˆå€‹åˆ¥é‘½å­”å ±è¡¨ ===")
        
        well_reports = {}
        well_ids = final_df['é‘½å­”ç·¨è™Ÿ'].unique()
        individual_dir = os.path.join(output_dir, 'individual_wells')
        
        try:
            # å°å…¥å ±è¡¨ç”Ÿæˆæ¨¡çµ„
            from .report import create_liquefaction_excel_from_dataframe, LiquefactionChartGenerator
            
            chart_generator = LiquefactionChartGenerator(
                n_chart_size=(5, 10),
                fs_chart_size=(5, 10)
            )
            
            for i, well_id in enumerate(well_ids, 1):
                print(f"é€²åº¦ [{i}/{len(well_ids)}] è™•ç†é‘½å­”ï¼š{well_id}")
                
                try:
                    # å»ºç«‹é‘½å­”è³‡æ–™å¤¾
                    well_dir = os.path.join(individual_dir, str(well_id))
                    os.makedirs(well_dir, exist_ok=True)
                    
                    # ç¯©é¸è©²é‘½å­”çš„è³‡æ–™
                    well_data = final_df[final_df['é‘½å­”ç·¨è™Ÿ'] == well_id].copy()
                    
                    if len(well_data) == 0:
                        print(f"  âš ï¸ é‘½å­” {well_id} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
                        continue
                    
                    # ç”ŸæˆExcelå ±è¡¨
                    current_date = datetime.now().strftime("%m%d")
                    excel_filename = f"{well_id}_æ¶²åŒ–åˆ†æå ±è¡¨_{current_date}.xlsx"
                    excel_filepath = os.path.join(well_dir, excel_filename)
                    
                    create_liquefaction_excel_from_dataframe(well_data, excel_filepath)
                    
                    # ç”Ÿæˆåœ–è¡¨
                    charts = []
                    chart1 = chart_generator.generate_depth_n_chart(well_data, well_id, well_dir)
                    if chart1:
                        charts.append(chart1)
                    
                    chart2 = chart_generator.generate_depth_fs_chart(well_data, well_id, well_dir)
                    if chart2:
                        charts.append(chart2)
                    
                    chart3 = chart_generator.generate_soil_column_chart(well_data, well_id, well_dir)
                    if chart3:
                        charts.append(chart3)
                    
                    well_reports[well_id] = {
                        'excel_report': excel_filepath,
                        'charts': charts,
                        'directory': well_dir
                    }
                    
                    print(f"  âœ… é‘½å­” {well_id} è™•ç†å®Œæˆ")
                    
                except Exception as e:
                    print(f"  âŒ è™•ç†é‘½å­” {well_id} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    continue
            
            return well_reports
            
        except ImportError as e:
            print(f"âŒ ç„¡æ³•å°å…¥å ±è¡¨ç”Ÿæˆæ¨¡çµ„ï¼š{e}")
            return {}
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå€‹åˆ¥é‘½å­”å ±è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return {}

    def generate_summary_report(self, 
                              analyzer, 
                              final_df: pd.DataFrame, 
                              output_dir: str) -> Optional[str]:
        """ç”ŸæˆLPIæ‘˜è¦å ±è¡¨"""
        print("\n=== ç”ŸæˆLPIæ‘˜è¦å ±è¡¨ ===")
        
        try:
            summary_dir = os.path.join(output_dir, 'summary')
            lpi_summary_file = analyzer.generate_lpi_summary_report(final_df, summary_dir)
            
            if lpi_summary_file:
                print("âœ… LPIæ‘˜è¦å ±è¡¨ç”Ÿæˆå®Œæˆ")
                return lpi_summary_file
            else:
                print("âŒ LPIæ‘˜è¦å ±è¡¨ç”Ÿæˆå¤±æ•—")
                return None
                
        except Exception as e:
            print(f"âŒ ç”ŸæˆLPIæ‘˜è¦å ±è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None

    def create_analysis_package(self, analysis_dir: str, method: str) -> str:
        """å°‡åˆ†æçµæœæ‰“åŒ…æˆZIPæª”æ¡ˆ"""
        print("\n=== æ‰“åŒ…åˆ†æçµæœ ===")
        
        try:
            # å‰µå»ºZIPæª”æ¡ˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_filename = f"{method}_analysis_results_{timestamp}.zip"
            zip_path = os.path.join(os.path.dirname(analysis_dir), zip_filename)
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(analysis_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, analysis_dir)
                        zipf.write(file_path, arcname)
            
            print(f"âœ… åˆ†æçµæœå·²æ‰“åŒ…è‡³ï¼š{zip_filename}")
            return zip_path
            
        except Exception as e:
            print(f"âŒ æ‰“åŒ…åˆ†æçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return None

    def analyze(self, 
               method: str,
               csv_content: bytes,
               shapefile_content: bytes = None,
               em_value: float = 72,
               unit_weight_unit: str = "t/m3") -> Dict[str, Any]:
        """
        ä¸»è¦åˆ†æå‡½æ•¸
        
        Args:
            method: åˆ†ææ–¹æ³• ('HBF' æˆ– 'NCEER')
            csv_content: CSVæª”æ¡ˆå…§å®¹
            shapefile_content: Shapefileæª”æ¡ˆå…§å®¹ (å¯é¸)
            em_value: SPTéŒ˜æ“Šèƒ½é‡æ•ˆç‡
            unit_weight_unit: çµ±é«”å–®ä½é‡å–®ä½ ('t/m3' æˆ– 'kN/m3')
            
        Returns:
            Dict: åŒ…å«åˆ†æçµæœå’Œæª”æ¡ˆè·¯å¾‘çš„å­—å…¸
        """
        
        print(f"\n{'='*80}")
        print(f"é–‹å§‹ {self.supported_methods.get(method, method)} æ¶²åŒ–åˆ†æ")
        print(f"{'='*80}")
        
        result = {
            'success': False,
            'method': method,
            'error_message': None,
            'analysis_directory': None,
            'zip_file_path': None,
            'summary': {}
        }
        
        try:
            # 1. é©—è­‰åˆ†ææ–¹æ³•
            if method not in self.supported_methods:
                result['error_message'] = f"ä¸æ”¯æ´çš„åˆ†ææ–¹æ³•ï¼š{method}"
                return result
            
            # 2. æº–å‚™è¼¸å…¥è³‡æ–™
            csv_path, error_msg = self.prepare_input_data(csv_content, method)
            if error_msg:
                result['error_message'] = error_msg
                return result
            
            # 3. æº–å‚™æ–·å±¤è³‡æ–™
            fault_gdf = self.prepare_fault_data(shapefile_content)
            
            # 4. å‰µå»ºåˆ†æç›®éŒ„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            analysis_dir = self.create_analysis_directory(method, timestamp)
            result['analysis_directory'] = analysis_dir
            
            print(f"åˆ†æçµæœå°‡å„²å­˜è‡³ï¼š{analysis_dir}")
            
            # 5. åŸ·è¡Œå°æ‡‰çš„åˆ†ææ–¹æ³•
            final_df = None
            lpi_summary = None
            analyzer = None
            
            if method == 'HBF':
                final_df, lpi_summary, error_msg = self.analyze_hbf(
                    csv_path, analysis_dir, fault_gdf, em_value, unit_weight_unit
                )
                if final_df is not None:
                    analyzer = HBF(default_em=em_value)
                    
            elif method == 'NCEER':
                final_df, lpi_summary, error_msg = self.analyze_nceer(
                    csv_path, analysis_dir, fault_gdf, em_value
                )
                if final_df is not None:
                    analyzer = NCEER(default_em=em_value)
            
            if error_msg:
                result['error_message'] = error_msg
                return result
            
            if final_df is None or analyzer is None:
                result['error_message'] = "åˆ†æå¤±æ•—ï¼šæœªç”¢ç”Ÿæœ‰æ•ˆçµæœ"
                return result
            
            # 6. ç”Ÿæˆå„ç¨®å ±è¡¨
            print("\n" + "="*60)
            print("=== å¾Œè™•ç†ï¼šç”Ÿæˆå ±è¡¨å’Œåœ–è¡¨ ===")
            print("="*60)
            
            # ç°¡åŒ–å ±è¡¨
            simplified_reports = self.generate_simplified_reports(analyzer, final_df, analysis_dir)
            
            # å€‹åˆ¥é‘½å­”å ±è¡¨
            individual_reports = self.generate_individual_well_reports(final_df, analysis_dir)
            
            # æ‘˜è¦å ±è¡¨
            summary_report = self.generate_summary_report(analyzer, final_df, analysis_dir)
            
            # 7. æ‰“åŒ…çµæœ
            zip_path = self.create_analysis_package(analysis_dir, method)
            if zip_path:
                result['zip_file_path'] = zip_path
            
            # 8. æ•´ç†åˆ†ææ‘˜è¦
            well_count = len(final_df['é‘½å­”ç·¨è™Ÿ'].unique()) if 'é‘½å­”ç·¨è™Ÿ' in final_df.columns else 0
            layer_count = len(final_df)
            
            result['summary'] = {
                'well_count': well_count,
                'layer_count': layer_count,
                'analysis_method': self.supported_methods[method],
                'em_value': em_value,
                'unit_weight_unit': unit_weight_unit,
                'fault_data_used': fault_gdf is not None,
                'simplified_reports': list(simplified_reports.keys()),
                'individual_reports_count': len(individual_reports),
                'has_summary_report': summary_report is not None,
                'lpi_summary': lpi_summary
            }
            
            # 9. æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                os.unlink(csv_path)
            except:
                pass
            
            result['success'] = True
            
            print(f"\n{'='*80}")
            print(f"ğŸ‰ {self.supported_methods[method]} åˆ†æå®Œæˆï¼")
            print(f"{'='*80}")
            print(f"åˆ†ææ‘˜è¦ï¼š")
            print(f"  é‘½å­”æ•¸é‡ï¼š{well_count}")
            print(f"  åœŸå±¤æ•¸é‡ï¼š{layer_count}")
            print(f"  Emå€¼ï¼š{em_value}%")
            print(f"  çµ±é«”å–®ä½é‡å–®ä½ï¼š{unit_weight_unit}")
            print(f"  ä½¿ç”¨æ–·å±¤è³‡æ–™ï¼š{'æ˜¯' if fault_gdf is not None else 'å¦'}")
            print(f"  çµæœç›®éŒ„ï¼š{analysis_dir}")
            if zip_path:
                print(f"  æ‰“åŒ…æª”æ¡ˆï¼š{os.path.basename(zip_path)}")
            print(f"{'='*80}")
            
            return result
            
        except Exception as e:
            result['error_message'] = f"åˆ†æéç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            print(f"âŒ {result['error_message']}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤ï¼š{traceback.format_exc()}")
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                if 'csv_path' in locals() and csv_path:
                    os.unlink(csv_path)
            except:
                pass
                
            return result

    def get_analysis_status(self, analysis_dir: str) -> Dict[str, Any]:
        """å–å¾—åˆ†æç‹€æ…‹å’Œçµæœæ‘˜è¦"""
        try:
            if not os.path.exists(analysis_dir):
                return {'exists': False}
            
            status = {'exists': True}
            
            # æª¢æŸ¥å„å­ç›®éŒ„çš„æª”æ¡ˆ
            subdirs = ['raw_results', 'simplified_reports', 'individual_wells', 'summary']
            for subdir in subdirs:
                subdir_path = os.path.join(analysis_dir, subdir)
                if os.path.exists(subdir_path):
                    files = [f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]
                    status[f'{subdir}_files'] = files
                else:
                    status[f'{subdir}_files'] = []
            
            return status
            
        except Exception as e:
            return {'exists': False, 'error': str(e)}

    def cleanup_old_results(self, days_old: int = 7):
        """æ¸…ç†èˆŠçš„åˆ†æçµæœ"""
        try:
            if not os.path.exists(self.results_base_dir):
                return
            
            import time
            current_time = time.time()
            cutoff_time = current_time - (days_old * 24 * 60 * 60)
            
            for item in os.listdir(self.results_base_dir):
                item_path = os.path.join(self.results_base_dir, item)
                if os.path.isdir(item_path):
                    if os.path.getctime(item_path) < cutoff_time:
                        shutil.rmtree(item_path)
                        print(f"å·²æ¸…ç†èˆŠçµæœç›®éŒ„ï¼š{item}")
                elif item.endswith('.zip'):
                    if os.path.getctime(item_path) < cutoff_time:
                        os.remove(item_path)
                        print(f"å·²æ¸…ç†èˆŠZIPæª”æ¡ˆï¼š{item}")
                        
        except Exception as e:
            print(f"æ¸…ç†èˆŠçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")


# å»ºè­°çš„Djangoè¨­å®š
"""
åœ¨ settings.py ä¸­åŠ å…¥ï¼š

# æ¶²åŒ–åˆ†æçµæœå„²å­˜ç›®éŒ„
LIQUEFACTION_RESULTS_DIR = os.path.join(MEDIA_ROOT, 'liquefaction_results')

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(LIQUEFACTION_RESULTS_DIR, exist_ok=True)

# å¯ä¸‹è¼‰æª”æ¡ˆçš„URLè¨­å®š
MEDIA_URL = '/media/'
MEDIA_ROOT = os.path.join(BASE_DIR, 'media')

åœ¨ urls.py ä¸­åŠ å…¥ï¼š
from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    # ... å…¶ä»–URLæ¨¡å¼
] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

ç›®éŒ„çµæ§‹å»ºè­°ï¼š
project_root/
â”œâ”€â”€ media/
â”‚   â””â”€â”€ liquefaction_results/
â”‚       â”œâ”€â”€ HBF_20241219_143022/
â”‚       â”‚   â”œâ”€â”€ raw_results/
â”‚       â”‚   â”œâ”€â”€ simplified_reports/
â”‚       â”‚   â”œâ”€â”€ individual_wells/
â”‚       â”‚   â”œâ”€â”€ charts/
â”‚       â”‚   â””â”€â”€ summary/
â”‚       â”œâ”€â”€ NCEER_20241219_143155/
â”‚       â””â”€â”€ analysis_results.zip
"""